# 감성 분석

> 감성분석 이론 정리

#### ▪️감성 분석에 원핫인코딩을 하지 않는 이유는?

- 원-핫 인코딩?
  - 문자열 값을 숫자형으로 인코딩하는 전처리 작업
  - scikit-learn에서 제공하는 머신러닝 알고리즘은 문자열 값을 입력 값으로 허락하지 않아서 필요
  - 한 개의 요소는 True, 나머지 요소는 False로 만들어 주는 기법 → 희소 표현, 희소 벡터
- 희소 벡터의 경우 단어 개수가 증가하면 벡터 차원이 한 없이 커짐 → 차원의 저주에 빠져 **속도 매우 느림**
- **단어 간 유사도 계산 불가**

<br>

#### ▪️희소 표현의 대안으로 나온 밀집 표현

- 희소 벡터처럼 벡터의 차원의 단어 집합의 크기로 상정하지 않음 
- **사용자가 설정한 값**으로 모든 단어의 벡터 표현의 차원을 맞춤 → (벡터의 차원이 조밀한) 밀집 벡터
- 단어 간 유사도 계산이 불가능한 희소 표현의 대안으로 밀집 표현의 한 종류인 **분산 표현** 등장

<br>

#### ▪️분산 표현

- **단어의 '의미'를 다차원 공간에 벡터화** 하는 방법
- 핵심 아이디어 : **'비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'**
- 강아지, 귀엽다, 애교 등 주로 함께 등장하는 단어들을 벡터화하면 의미적으로 가까운 단어가 됨

<br>

```markdown
💡결론

희소 표현이 고차원에 각 차원이 분리된 표현이라면, 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현하는 방법이다. 분산 표현의 방법을 사용하면 단어 간 유사도를 계산할 수 있다.
```

<br>

#### ▪️단어 임베딩 방법론

> 단어를 밀집 벡터의 형태로 표현하는 방법
>
> Word2Vec과 Glove가 많이 사용됨

| 벡터화 방법론                              | 설명                                                         | 대표 모델    | 장점                      | 단점                                                  |
| ------------------------------------------ | ------------------------------------------------------------ | ------------ | ------------------------- | ----------------------------------------------------- |
| 카운팅 기반의 방법                         | 각 문서에서의 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입력으로 받아 차원을 축소하여 잠재된 의미를 끌어내는 방법론 | LSA          | 전체적인 통계 정보를 고려 | 왕:남자 = 여왕:? 같은 단어 의미의 유추 작업 성능 저하 |
| 예측 기반의 방법                           | 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반의 방법론 | **Word2Vec** | 유추작업 성능 뛰어남      | 전체적인 통계 정보를 고려 X                           |
| 카운팅 기반과 예측 기반 모두 사용하는 방법 | LSA의 메커니즘이었던 카운트 기반의 방법과 Word2Vec의 메커니즘이었던 예측 기반의 방법론 두 가지를 모두 사용 | **Glove**    | LSA, Wokd2Vec 단점 보완   |                                                       |

<br>

#### ▪️Word2Vec

- 단어를 벡터로 변환(Word to Vector) → 변환해야 유사도 등의 계산 가능 즉, 분산 표현이 가능해짐

- 딥러닝은 입력층과 출력층 사이의 은닉층 개수가 충분히 쌓인 신경망을 말함
- Word2Vec은 2개의 레이어 뿐이므로 딥러닝이라 볼 수 없고, 얕은 신경망이라고 부를 수 있음

- 크게 CBOW 방식과 Skip-gram 방식으로 구분
- 여러 논문에서 성능 비교를 진행했을 때, **전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있음** 

| 방식                                   | CBOW(Continuous Bag of Words)                                | Skip-Gram                                                    |
| -------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 정의                                   | 주변 단어들로 중간에 있는 단어를 예측                        | 중간에 있는 단어로 주변 단어들을 예측                        |
| 예문<br />"The fat cat sat on the mat" | {"The", "fat", "cat", "on", "the", "mat"}으로부터 sat을 예측 | sat으로부터 {"The", "fat", "cat", "on", "the", "mat"}을 예측 |

<br>

#### ▪️Glove

- 카운트 기반(LSA)과 예측 기반(Word2Vec) 단점을 보완한다는 목적으로 등장한 방법론
- 연구에 따르면, Word2Vec과 Glove 중에 더 뛰어난 것은 없고, **두 가지를 모두 사용하여 성능을 비교해보는 것이 좋음**

